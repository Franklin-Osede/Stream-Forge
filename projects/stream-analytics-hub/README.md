# ğŸ“Š Stream Analytics Hub

> **KSQLDB + Flink para agregaciones, dashboards de KPIs.**

## ğŸ“‹ **DescripciÃ³n**

Stream Analytics Hub es el centro de procesamiento de streams del ecosistema StreamForge. Utiliza KSQLDB y Apache Flink para realizar agregaciones en tiempo real, calcular KPIs y generar insights de negocio.

## ğŸ› ï¸ **Stack TecnolÃ³gico**

- **Stream Processing**: KSQLDB + Apache Flink
- **Backend**: Java + Spring Boot
- **Streaming**: Kafka + Kafka Streams
- **Observabilidad**: Prometheus + Grafana
- **ContainerizaciÃ³n**: Docker

## ğŸš€ **CaracterÃ­sticas**

- âœ… Agregaciones en tiempo real (sum, count, avg, min, max)
- âœ… Ventanas deslizantes y de tiempo
- âœ… CÃ¡lculo de KPIs automÃ¡tico
- âœ… Dashboards interactivos
- âœ… Alertas basadas en umbrales
- âœ… ExportaciÃ³n de datos a mÃºltiples formatos
- âœ… API REST para consultas

## ğŸ“Š **Arquitectura**

```mermaid
graph LR
    A[Kafka Topics] --> B[KSQLDB Streams]
    B --> C[Flink Jobs]
    C --> D[Agregaciones]
    D --> E[KPI Calculator]
    E --> F[Dashboard API]
    E --> G[Alert Engine]
    F --> H[Grafana Dashboards]
```

## ğŸ”§ **ConfiguraciÃ³n**

### **Variables de Entorno**

```bash
# Kafka
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC_INPUT=blockchain-events
KAFKA_TOPIC_OUTPUT=analytics-results

# KSQLDB
KSQLDB_SERVER=http://ksqldb:8088
KSQLDB_QUERY_TIMEOUT=30000

# Flink
FLINK_JOBMANAGER_RPC_ADDRESS=flink-jobmanager
FLINK_TASKMANAGER_NUMBEROFTASKSLOTS=4

# Observabilidad
PROMETHEUS_PORT=9091
LOG_LEVEL=info
```

### **Endpoints de API**

```yaml
GET  /api/v1/analytics/summary     # Resumen de KPIs
GET  /api/v1/analytics/trends      # Tendencias temporales
GET  /api/v1/analytics/alerts      # Alertas activas
POST /api/v1/analytics/query       # Consultas personalizadas
GET  /api/v1/metrics               # MÃ©tricas
GET  /api/v1/health                # Health check
```

## ğŸš€ **Inicio RÃ¡pido**

```bash
# Instalar dependencias
mvn clean install

# Configurar variables de entorno
cp .env.example .env

# Levantar en desarrollo
make up

# Ver logs
make logs
```

## ğŸ“ˆ **KPIs Disponibles**

### **Blockchain Analytics**
- **Transacciones por minuto/hora/dÃ­a**
- **Volumen total por perÃ­odo**
- **Top wallets por actividad**
- **Gas price trends**
- **Network congestion metrics**

### **AI/ML Analytics**
- **Model accuracy trends**
- **Prediction confidence scores**
- **Processing latency metrics**
- **Error rates by model**

### **IoT Analytics**
- **Sensor data aggregation**
- **Device health metrics**
- **Environmental trends**
- **Anomaly detection rates**

## ğŸ§ª **Testing**

```bash
# Tests unitarios
mvn test

# Tests de integraciÃ³n
mvn test -Dtest=IntegrationTest

# Tests de performance
mvn test -Dtest=PerformanceTest

# Coverage
mvn jacoco:report
```

## ğŸ“š **API Documentation**

### **Obtener Resumen de KPIs**

```bash
curl -X GET "http://localhost:8081/api/v1/analytics/summary?period=1h&metrics=transactions,volume"
```

### **Consultar Tendencias**

```bash
curl -X GET "http://localhost:8081/api/v1/analytics/trends?metric=transactions&window=1h&granularity=5m"
```

### **Consulta Personalizada**

```bash
curl -X POST http://localhost:8081/api/v1/analytics/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "SELECT COUNT(*) as total FROM blockchain_events WHERE timestamp > NOW() - INTERVAL 1 HOUR",
    "format": "json"
  }'
```

## ğŸ” **Monitoreo**

### **Health Check**

```bash
curl http://localhost:8081/api/v1/health
```

### **MÃ©tricas**

```bash
curl http://localhost:8081/api/v1/metrics
```

### **Dashboard Grafana**

El proyecto incluye dashboards pre-configurados para:
- KPIs en tiempo real
- Tendencias y patrones
- Alertas y notificaciones
- Performance del sistema

## ğŸ³ **Docker**

```bash
# Construir imagen
docker build -t streamforge/stream-analytics-hub .

# Ejecutar contenedor
docker run -p 8081:8080 \
  -e KAFKA_BROKERS=localhost:9092 \
  -e KSQLDB_SERVER=http://ksqldb:8088 \
  streamforge/stream-analytics-hub
```

## ğŸ“ **Estructura del Proyecto**

```
stream-analytics-hub/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/java/
â”‚   â”‚   â”œâ”€â”€ controllers/     # REST controllers
â”‚   â”‚   â”œâ”€â”€ services/        # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ processors/      # Flink processors
â”‚   â”‚   â”œâ”€â”€ ksql/           # KSQL queries
â”‚   â”‚   â””â”€â”€ config/         # ConfiguraciÃ³n
â”œâ”€â”€ flink-jobs/             # Jobs de Flink
â”œâ”€â”€ ksql-queries/           # Queries KSQL
â”œâ”€â”€ dashboards/             # Dashboards Grafana
â”œâ”€â”€ tests/                  # Tests
â””â”€â”€ docker/                # ConfiguraciÃ³n Docker
```

## ğŸ¯ **Casos de Uso**

### **Fintech**
- Monitoreo de transacciones en tiempo real
- DetecciÃ³n de patrones de fraude
- CÃ¡lculo de mÃ©tricas de riesgo

### **E-commerce**
- AnÃ¡lisis de comportamiento de usuarios
- MÃ©tricas de conversiÃ³n
- OptimizaciÃ³n de inventario

### **IoT**
- AgregaciÃ³n de datos de sensores
- Monitoreo de dispositivos
- AnÃ¡lisis de eficiencia energÃ©tica

## ğŸ”§ **KSQL Queries Ejemplo**

```sql
-- Crear stream de transacciones
CREATE STREAM blockchain_transactions (
    transaction_id VARCHAR,
    from_address VARCHAR,
    to_address VARCHAR,
    value DECIMAL,
    timestamp BIGINT
) WITH (
    KAFKA_TOPIC='blockchain-events',
    VALUE_FORMAT='JSON'
);

-- Calcular volumen por hora
CREATE TABLE hourly_volume AS
SELECT 
    WINDOWSTART as window_start,
    WINDOWEND as window_end,
    SUM(value) as total_volume,
    COUNT(*) as transaction_count
FROM blockchain_transactions
WINDOW TUMBLING (SIZE 1 HOUR)
GROUP BY WINDOWSTART, WINDOWEND;
```

## ğŸ¤ **Contribuir**

1. Fork el proyecto
2. Crea tu feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ **Licencia**

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

---

**Parte del ecosistema StreamForge** ğŸš€
